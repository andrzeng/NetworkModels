{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ad74b7-8cb7-462f-b966-2efc53b469ae",
   "metadata": {},
   "source": [
    "2023-1-2\n",
    "\n",
    "Changes:\n",
    "- mandate that the embedding dimension equals the dimension of the output of the self-attention layer and  the subsequent FF layers (due to skip-connection additions)\n",
    "- removed the custom FFlayer class \n",
    "\n",
    "Todo:\n",
    "- implement positional embeddings\n",
    "- implement decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df2b066-6d8e-4a6a-bdac-14975399a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8493e26-e3cc-4634-823d-d48fef2e4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, qkv_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.qkv_dim = qkv_dim\n",
    "        \n",
    "        self.query_matrix = Parameter(torch.rand(embedding_dim, qkv_dim), requires_grad=True)\n",
    "        self.key_matrix = Parameter(torch.rand(embedding_dim, qkv_dim), requires_grad=True)\n",
    "        self.value_matrix = Parameter(torch.rand(embedding_dim, qkv_dim), requires_grad=True)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        queries = torch.matmul(embeddings, self.query_matrix)\n",
    "        keys = torch.matmul(embeddings, self.key_matrix)\n",
    "        values = torch.matmul(embeddings, self.value_matrix)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.T)\n",
    "        scaled_attention_scores = attention_scores/math.sqrt(self.qkv_dim)\n",
    "        softmaxed_attention_scores = torch.softmax(scaled_attention_scores, dim=1)\n",
    "        #print(softmaxed_attention_scores, values)\n",
    "        return torch.matmul(softmaxed_attention_scores, values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e66f7082-421e-43ba-97c4-dc5db267f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 qkv_dim, \n",
    "                 #out_dim, Note: the out_dim must match the embedding_dim, since there is a skip connection (the input to the MHA will be added to the output of the MHA)\n",
    "                 n_heads=8):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.qkv_dim = qkv_dim\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.attention_heads = [SelfAttention(embedding_dim, qkv_dim) for i in range(n_heads)]\n",
    "        self.after_concat_multiplier_matrix = Parameter(torch.rand(qkv_dim*n_heads, embedding_dim), requires_grad=True)\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        indiv_att_outs = [attention_head(embeddings) for attention_head in self.attention_heads]\n",
    "        concat_outs = torch.concat(indiv_att_outs,  dim=1)\n",
    "        recombined = torch.matmul(concat_outs, self.after_concat_multiplier_matrix)\n",
    "        \n",
    "        return recombined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73b3fdb1-e4e4-4f4d-89ae-f0076d3f2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim, #dimension of the word embeddings \n",
    "                 qkv_dim, #dimension of the query, key, and value vectors\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.multiheadAttention1 = MultiheadAttention(embedding_dim, qkv_dim)\n",
    "        self.feedforwardLayer1 = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        \n",
    "        self.multiheadAttention2 = MultiheadAttention(embedding_dim, qkv_dim)\n",
    "        self.feedforwardLayer2 = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.qkv_dim = qkv_dim\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        MHA1_output = self.multiheadAttention1(embeddings)\n",
    "    \n",
    "        #Perform the skip connection addition, then perform a layer normalization:\n",
    "        layernormed_output1 = self.layer_norm(embeddings + MHA1_output)\n",
    "    \n",
    "        #Feed the layer normed output into the feedforward layers... but before we do, create a deep copy of them since we will perform another skip connection addition after:\n",
    "        FF1_output = self.feedforwardLayer1(layernormed_output1)\n",
    "        \n",
    "        layernormed_output2 = self.layer_norm(layernormed_output1 + FF1_output) #these will be the embeddings that we will pass to the second self-attention layer\n",
    "        #Do the same for the second encoder:\n",
    "        MHA2_output = self.multiheadAttention2(layernormed_output2)\n",
    "        layernormed_output3 = self.layer_norm(layernormed_output2 + MHA2_output)\n",
    "        FF2_output = self.feedforwardLayer2(layernormed_output2)\n",
    "        \n",
    "        final_encoder_output = self.layer_norm(FF2_output + layernormed_output3)\n",
    "        return final_encoder_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e2c341bf-7f7a-4a9f-bf89-e4289f905c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = TransformerEncoder(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8dd937f3-ac9d-4cc2-ba15-d80932e6e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_word_embeddings = torch.rand(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fdb36c60-4594-4643-9e59-e6b274bb0b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0838,  0.7952, -1.6590,  0.7800],\n",
       "        [ 0.1211,  0.7543, -1.6692,  0.7938]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(fake_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d83f9-11cd-480e-97f3-9f4e93214a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
