{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf2db66-75fa-44b6-b909-dd41be7e6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bab01544-2d74-4ed7-9582-28bed17eb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class generates correlated memory vectors as decribed in (Benna, Fusi; 2021)\n",
    "class CorrelatedPatterns():\n",
    "    def __init__(self, \n",
    "                 L, #Length of each memory vector \n",
    "                 p, #Number of ancestors\n",
    "                 k, #Number of children per ancestor\n",
    "                 gamma): #Average overlap between child and ancestor. A value of one means each child is identical to its ancestor,\n",
    "                        #while a value of zero means each child is completely different from its ancestor.\n",
    "        self.L = L\n",
    "        self.p = p\n",
    "        self.k = k\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #Create three arrays to store the ancestor vectors, the descendant (child) vectors, and the difference vectors\n",
    "        self.ancestors = []\n",
    "        self.descendants = []\n",
    "        self.differences = []\n",
    "        \n",
    "        #For purposes of PyTorch dataset creation, we will create two new lists that do not themselves contain lists\n",
    "        self.descendants_singlelist = []\n",
    "        self.differences_singlelist = []\n",
    "        \n",
    "        for _ancestorIndex in range(p):\n",
    "            \n",
    "            #Each ancestor is initialized randomly\n",
    "            ancestor = np.random.choice((0,1), size=(L))\n",
    "            self.ancestors.append(np.array(ancestor))\n",
    "            \n",
    "            self.descendants.append([])\n",
    "            #Initialize k descendants\n",
    "            for _descendantIndex in range(k):\n",
    "                descendant = torch.tensor([])\n",
    "                for __i in range(len(ancestor)):\n",
    "                    \n",
    "                    #With probability 1-gamma, the descendant memory is corrupted at this bit. \n",
    "                    if(random.uniform(0,1) < 1-gamma):\n",
    "                        descendant = torch.cat((descendant, torch.tensor([0 if random.uniform(0,1) < 0.5 else 1])))\n",
    "                    else: #Otherwise, the ancestor's memory at this bit is copied to the descendant.\n",
    "                        descendant = torch.cat((descendant, torch.tensor([ancestor[__i]])))\n",
    "                \n",
    "                #Save the memory\n",
    "                self.descendants[_ancestorIndex].append(descendant.clone().detach())\n",
    "                self.descendants_singlelist.append(descendant.clone().detach().reshape(1,-1))\n",
    "            \n",
    "            #Calculate the differences between the ancestor vectors and the child vectors\n",
    "            self.differences.append([])\n",
    "            for _descendantIndex in range(k):\n",
    "                self.differences[_ancestorIndex].append(torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex])\n",
    "                self.differences_singlelist.append((torch.tensor(self.ancestors[_ancestorIndex]) - self.descendants[_ancestorIndex][_descendantIndex]).reshape(1,-1))\n",
    "                \n",
    "        self.descendants_singlelist = torch.cat(self.descendants_singlelist)\n",
    "        self.differences_singlelist = torch.cat(self.differences_singlelist)\n",
    "\n",
    "#This subclass inherits the PyTorch Dataset class in order to create datasets of correlated memory.\n",
    "class SensoryData(Dataset):\n",
    "    def __init__(self, \n",
    "                 L,      #Length of each sample\n",
    "                 p,      #Number of parents\n",
    "                 k,      #Number of children per parent \n",
    "                 gamma   #Overlap between parent and children (1=identical, 0=no overlap)\n",
    "                ):\n",
    "        super().__init__()\n",
    "        c = CorrelatedPatterns(L, p, k, gamma)\n",
    "        memories = c.descendants_singlelist\n",
    "        \n",
    "        #Grab the memories generated by CorrelatedPatterns()\n",
    "        self.data = memories\n",
    "        self.x = memories\n",
    "        self.y = memories\n",
    "        self.n_samples = memories.shape[0]\n",
    "    \n",
    "    #Implement necessary helper functions\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc9fc9e-3d6b-4927-950c-6b952399c935",
   "metadata": {},
   "source": [
    "Ideas to try:\n",
    "- Fast weights that modify a fixed subset of the main weights only\n",
    "- Meta-meta fast weights\n",
    "- Fast weights that gate plasticity instead of set weights (connect with Dendritic Gated Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eec5adcd-e769-4ad3-b4e6-51fc0954fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastWeightAE(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_inputs=10,\n",
    "                 n_hiddens=5,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fast_weights = Parameter(torch.rand(n_inputs, n_inputs*n_hiddens), requires_grad=True)\n",
    "        self.eweight = Parameter(torch.rand(n_hiddens, n_inputs), requires_grad=True)\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hiddens = n_hiddens\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Limitation: X can only be a single sample\n",
    "        \n",
    "        #Generate weights using fast weight network\n",
    "        generated_weights = F.linear(X, self.fast_weights.T)\n",
    "        generated_weights = generated_weights.reshape_as(self.eweight.data)\n",
    "        self.eweight.data = generated_weights\n",
    "        \n",
    "        #Run the input through the main network\n",
    "        encoded = F.linear(X, self.eweight)\n",
    "        encoded = F.relu(encoded)\n",
    "        \n",
    "        decoded = F.linear(encoded, self.eweight.T)\n",
    "        \n",
    "        return decoded        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f1d3ba0c-e33f-4ace-aee7-4bf2deb9ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, #Dataloader\n",
    "          model,  #Model to be trained\n",
    "          loss_function, #Loss function\n",
    "          optimizer, #Optimizer\n",
    "          n_epochs=100 #number of epochs\n",
    "         ):\n",
    "    \n",
    "    #Toggle to training mode\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        #Iterate through the DataLoader's batches\n",
    "        for batch, (X, y) in enumerate(loader):\n",
    "            #Get the model's prediction of the input\n",
    "            predicted_y = model(X)\n",
    "            \n",
    "            #Calculate the loss\n",
    "            loss = loss_function(predicted_y, y)\n",
    "            total_loss += loss\n",
    "            \n",
    "            #Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            #Update the optimizer\n",
    "            optimizer.step()\n",
    "        print(f'Average loss: {total_loss/len(loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4a65c4ae-3990-4774-a4b5-4df52fdf05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "dataset = SensoryData(10, 5,7,0.5)\n",
    "loader = DataLoader(dataset, batch_size=1)\n",
    "model = FastWeightAE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1020438a-8419-4a57-bef1-603d1d54aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n",
      "Average loss: 63227.13671875\n"
     ]
    }
   ],
   "source": [
    "train(loader, model, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8888f449-dfec-4cda-be24-0eea3eb837df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f2fc78ae-e623-433c-bf8a-1a92fea3312f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[141.4532, 191.2325, 184.4556, 153.1359, 185.6811, 181.9413, 194.4612,\n",
       "         176.1580, 152.2388, 166.7613]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6fccc686-c236-476e-9c6c-2f4e5465f778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 1., 1., 1., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daeb7fe-53de-42bd-9129-3332f6bd7853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
